{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40ed66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa8e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-24468020-75e8-4e95-8aec-b257db77fe57;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 235ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-24468020-75e8-4e95-8aec-b257db77fe57\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/6ms)\n",
      "25/06/20 12:01:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"App\") \\\n",
    "    .master(\"spark://master2:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"AKIA46QT33XSZVW7UWV3\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"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\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")# total across executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6e1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "SPARK_UI = \"http://172.31.16.136:4040\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d045a",
   "metadata": {},
   "source": [
    "## Von s3 lesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3227ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 10 braucht 0.21 Sekunden\n",
      "Job ID: 11 braucht 0.20 Sekunden\n",
      "Job ID: 12 braucht 0.20 Sekunden\n",
      "Job ID: 13 braucht 0.20 Sekunden\n",
      "Job ID: 14 braucht 0.21 Sekunden\n",
      "Job ID: 15 braucht 0.19 Sekunden\n",
      "Job ID: 16 braucht 0.20 Sekunden\n",
      "Job ID: 17 braucht 0.18 Sekunden\n",
      "Job ID: 18 braucht 0.20 Sekunden\n",
      "Job ID: 19 braucht 0.20 Sekunden\n",
      "[0.214, 0.204, 0.196, 0.196, 0.206, 0.188, 0.202, 0.182, 0.197, 0.2]\n",
      "Zeit für die Ausführung der Zelle: 5.74 seconds\n",
      "Mittelwert aller Spark Jobs: 0.20 seconds\n",
      "Median aller Spark Jobs: 0.20 seconds\n",
      "Sdt aller Spark Jobs: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "lista = []\n",
    "\n",
    "start_all = time.time()\n",
    "for i in range(10):\n",
    "    dfrh= spark.read.parquet(\"s3a://bucket25db/parquet\")\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()[0]\n",
    "    start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    timer = (end - start).total_seconds()\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    latest_job = sorted(job_info, key=lambda j: j[\"submissionTime\"], reverse=True)[0]\n",
    "    lista.append(timer)\n",
    "    print(f\"Job ID: {latest_job['jobId']} braucht {timer:.2f} Sekunden\")\n",
    "end_all = time.time() \n",
    "\n",
    "print(lista)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(lista):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(lista):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(lista):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f31a9",
   "metadata": {},
   "source": [
    "## Von hdfs lesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e7d9b-0e6d-4269-9057-7cf2a2cee7d9",
   "metadata": {},
   "source": [
    "10 Wiederholungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa0b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 30 braucht 0.05 Sekunden\n",
      "Job ID: 31 braucht 0.06 Sekunden\n",
      "Job ID: 32 braucht 0.05 Sekunden\n",
      "Job ID: 33 braucht 0.04 Sekunden\n",
      "Job ID: 34 braucht 0.04 Sekunden\n",
      "Job ID: 35 braucht 0.04 Sekunden\n",
      "Job ID: 36 braucht 0.04 Sekunden\n",
      "Job ID: 37 braucht 0.04 Sekunden\n",
      "Job ID: 38 braucht 0.04 Sekunden\n",
      "Job ID: 39 braucht 0.05 Sekunden\n",
      "[0.046, 0.056, 0.05, 0.037, 0.042, 0.039, 0.04, 0.036, 0.039, 0.048]\n",
      "Zeit für die Ausführung der Zelle: 1.23 seconds\n",
      "Mittelwert aller Spark Jobs: 0.04 seconds\n",
      "Median aller Spark Jobs: 0.04 seconds\n",
      "Sdt aller Spark Jobs: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "listh10 = []\n",
    "\n",
    "start_all = time.time()\n",
    "for i in range(10):\n",
    "    dfrh= spark.read.parquet(\"hdfs:///data/data\")\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()[0]\n",
    "    start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    timer = (end - start).total_seconds()\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    latest_job = sorted(job_info, key=lambda j: j[\"submissionTime\"], reverse=True)[0]\n",
    "    listh10.append(timer)\n",
    "    print(f\"Job ID: {latest_job['jobId']} braucht {timer:.2f} Sekunden\")\n",
    "end_all = time.time() \n",
    "\n",
    "print(listh10)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(listh10):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(listh10):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(listh10):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778814b3-5527-490d-88d3-d295a83f9b00",
   "metadata": {},
   "source": [
    "100 Wiederholungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b67bb6-7a93-4f89-b719-68267851eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 40 took 0.05 seconds\n",
      "Job ID: 41 took 0.04 seconds\n",
      "Job ID: 42 took 0.05 seconds\n",
      "Job ID: 43 took 0.04 seconds\n",
      "Job ID: 44 took 0.04 seconds\n",
      "Job ID: 45 took 0.04 seconds\n",
      "Job ID: 46 took 0.03 seconds\n",
      "Job ID: 47 took 0.03 seconds\n",
      "Job ID: 48 took 0.04 seconds\n",
      "Job ID: 49 took 0.04 seconds\n",
      "Job ID: 50 took 0.03 seconds\n",
      "Job ID: 51 took 0.04 seconds\n",
      "Job ID: 52 took 0.04 seconds\n",
      "Job ID: 53 took 0.03 seconds\n",
      "Job ID: 54 took 0.04 seconds\n",
      "Job ID: 55 took 0.04 seconds\n",
      "Job ID: 56 took 0.04 seconds\n",
      "Job ID: 57 took 0.03 seconds\n",
      "Job ID: 58 took 0.04 seconds\n",
      "Job ID: 59 took 0.05 seconds\n",
      "Job ID: 60 took 0.04 seconds\n",
      "Job ID: 61 took 0.03 seconds\n",
      "Job ID: 62 took 0.03 seconds\n",
      "Job ID: 63 took 0.03 seconds\n",
      "Job ID: 64 took 0.04 seconds\n",
      "Job ID: 65 took 0.03 seconds\n",
      "Job ID: 66 took 0.03 seconds\n",
      "Job ID: 67 took 0.03 seconds\n",
      "Job ID: 68 took 0.03 seconds\n",
      "Job ID: 69 took 0.03 seconds\n",
      "Job ID: 70 took 0.03 seconds\n",
      "Job ID: 71 took 0.05 seconds\n",
      "Job ID: 72 took 0.06 seconds\n",
      "Job ID: 73 took 0.03 seconds\n",
      "Job ID: 74 took 0.04 seconds\n",
      "Job ID: 75 took 0.03 seconds\n",
      "Job ID: 76 took 0.04 seconds\n",
      "Job ID: 77 took 0.06 seconds\n",
      "Job ID: 78 took 0.04 seconds\n",
      "Job ID: 79 took 0.03 seconds\n",
      "Job ID: 80 took 0.03 seconds\n",
      "Job ID: 81 took 0.04 seconds\n",
      "Job ID: 82 took 0.03 seconds\n",
      "Job ID: 83 took 0.03 seconds\n",
      "Job ID: 84 took 0.03 seconds\n",
      "Job ID: 85 took 0.05 seconds\n",
      "Job ID: 86 took 0.04 seconds\n",
      "Job ID: 87 took 0.03 seconds\n",
      "Job ID: 88 took 0.03 seconds\n",
      "Job ID: 89 took 0.04 seconds\n",
      "Job ID: 90 took 0.05 seconds\n",
      "Job ID: 91 took 0.03 seconds\n",
      "Job ID: 92 took 0.05 seconds\n",
      "Job ID: 93 took 0.04 seconds\n",
      "Job ID: 94 took 0.04 seconds\n",
      "Job ID: 95 took 0.04 seconds\n",
      "Job ID: 96 took 0.03 seconds\n",
      "Job ID: 97 took 0.04 seconds\n",
      "Job ID: 98 took 0.03 seconds\n",
      "Job ID: 99 took 0.03 seconds\n",
      "Job ID: 100 took 0.05 seconds\n",
      "Job ID: 101 took 0.03 seconds\n",
      "Job ID: 102 took 0.04 seconds\n",
      "Job ID: 103 took 0.05 seconds\n",
      "Job ID: 104 took 0.05 seconds\n",
      "Job ID: 105 took 0.03 seconds\n",
      "Job ID: 106 took 0.03 seconds\n",
      "Job ID: 107 took 0.03 seconds\n",
      "Job ID: 108 took 0.03 seconds\n",
      "Job ID: 109 took 0.05 seconds\n",
      "Job ID: 110 took 0.04 seconds\n",
      "Job ID: 111 took 0.03 seconds\n",
      "Job ID: 112 took 0.03 seconds\n",
      "Job ID: 113 took 0.03 seconds\n",
      "Job ID: 114 took 0.03 seconds\n",
      "Job ID: 115 took 0.06 seconds\n",
      "Job ID: 116 took 0.03 seconds\n",
      "Job ID: 117 took 0.03 seconds\n",
      "Job ID: 118 took 0.03 seconds\n",
      "Job ID: 119 took 0.03 seconds\n",
      "Job ID: 120 took 0.04 seconds\n",
      "Job ID: 121 took 0.03 seconds\n",
      "Job ID: 122 took 0.03 seconds\n",
      "Job ID: 123 took 0.03 seconds\n",
      "Job ID: 124 took 0.04 seconds\n",
      "Job ID: 125 took 0.03 seconds\n",
      "Job ID: 126 took 0.03 seconds\n",
      "Job ID: 127 took 0.05 seconds\n",
      "Job ID: 128 took 0.03 seconds\n",
      "Job ID: 129 took 0.04 seconds\n",
      "Job ID: 130 took 0.03 seconds\n",
      "Job ID: 131 took 0.03 seconds\n",
      "Job ID: 132 took 0.03 seconds\n",
      "Job ID: 133 took 0.03 seconds\n",
      "Job ID: 134 took 0.04 seconds\n",
      "Job ID: 135 took 0.03 seconds\n",
      "Job ID: 136 took 0.03 seconds\n",
      "Job ID: 137 took 0.03 seconds\n",
      "Job ID: 138 took 0.03 seconds\n",
      "Job ID: 139 took 0.03 seconds\n",
      "[0.047, 0.038, 0.051, 0.039, 0.035, 0.035, 0.034, 0.034, 0.042, 0.042, 0.034, 0.035, 0.035, 0.034, 0.037, 0.04, 0.039, 0.033, 0.035, 0.049, 0.036, 0.034, 0.032, 0.033, 0.036, 0.032, 0.032, 0.033, 0.031, 0.031, 0.033, 0.048, 0.064, 0.033, 0.035, 0.033, 0.044, 0.059, 0.045, 0.032, 0.031, 0.044, 0.034, 0.033, 0.031, 0.048, 0.039, 0.029, 0.029, 0.043, 0.051, 0.03, 0.047, 0.042, 0.04, 0.036, 0.031, 0.044, 0.031, 0.03, 0.046, 0.032, 0.042, 0.048, 0.054, 0.031, 0.03, 0.029, 0.029, 0.048, 0.038, 0.032, 0.028, 0.028, 0.03, 0.055, 0.031, 0.03, 0.029, 0.031, 0.045, 0.034, 0.03, 0.031, 0.043, 0.031, 0.031, 0.049, 0.031, 0.036, 0.03, 0.029, 0.029, 0.028, 0.04, 0.034, 0.029, 0.029, 0.028, 0.029]\n",
      "Zeit für die Ausführung der Zelle: 10.39 seconds\n",
      "Mittelwert aller Spark Jobs: 0.04 seconds\n",
      "Median aller Spark Jobs: 0.03 seconds\n",
      "Sdt aller Spark Jobs: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "listh100 = []\n",
    "\n",
    "start_all = time.time()\n",
    "for i in range(100):\n",
    "    dfrh= spark.read.parquet(\"hdfs:///data/data\")\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()[0]\n",
    "    start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    timer = (end - start).total_seconds()\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    latest_job = sorted(job_info, key=lambda j: j[\"submissionTime\"], reverse=True)[0]\n",
    "    listh100.append(timer)\n",
    "    print(f\"Job ID: {latest_job['jobId']} took {timer:.2f} seconds\")\n",
    "end_all = time.time()\n",
    "\n",
    "print(listh100)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(listh100):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(listh100):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(listh100):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d06c70",
   "metadata": {},
   "source": [
    "## Count testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrh= spark.read.parquet(\"hdfs:///data/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "520ca32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 12:02:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "dfrh.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f9606",
   "metadata": {},
   "source": [
    "Einfacher select und count, ohne Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25658c67-05eb-4a7f-bb03-38a3fc7f84af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 145 took 0.02s\n",
      "  Included Job 144 took 7.63s\n",
      "  Included Job 143 took 6.24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 148 took 0.04s\n",
      "  Included Job 147 took 7.31s\n",
      "  Included Job 146 took 5.60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 151 took 0.02s\n",
      "  Included Job 150 took 7.48s\n",
      "  Included Job 149 took 5.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 154 took 0.02s\n",
      "  Included Job 153 took 7.38s\n",
      "  Included Job 152 took 5.38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 157 took 0.02s\n",
      "  Included Job 156 took 7.43s\n",
      "  Included Job 155 took 5.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 160 took 0.02s\n",
      "  Included Job 159 took 7.50s\n",
      "  Included Job 158 took 5.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 163 took 0.02s\n",
      "  Included Job 162 took 7.22s\n",
      "  Included Job 161 took 5.21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 166 took 0.02s\n",
      "  Included Job 165 took 7.41s\n",
      "  Included Job 164 took 5.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 169 took 0.04s\n",
      "  Included Job 168 took 7.34s\n",
      "  Included Job 167 took 5.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 172 took 0.02s\n",
      "  Included Job 171 took 7.64s\n",
      "  Included Job 170 took 5.19s\n",
      "Average time over 10 queries: 12.93 seconds\n",
      "[13.893, 12.951, 13.09, 12.780000000000001, 12.786999999999999, 12.797, 12.452, 12.678, 13.067, 12.847999999999999]\n",
      "Zeit für die Ausführung der Zelle: 150.51 seconds\n",
      "Mittelwert aller Spark Jobs: 12.93 seconds\n",
      "Median aller Spark Jobs: 12.82 seconds\n",
      "Sdt aller Spark Jobs: 0.36 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "listcount = []\n",
    "\n",
    "start_all = time.time()\n",
    "def get_all_jobs():\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    return job_info\n",
    "\n",
    "for i in range(10):\n",
    "    # Step 1: Snapshot of current jobs\n",
    "    jobs_before = {j[\"jobId\"] for j in get_all_jobs()}\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT gbifID, COUNT(gbifID) AS count\n",
    "    FROM my_table\n",
    "    WHERE gbifID IS NOT NULL\n",
    "    GROUP BY gbifID\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Create Spark DataFrame\n",
    "    spark_df = spark.sql(query)\n",
    "\n",
    "# Show the result in Spark (default top 20 rows)\n",
    "    spark_df.count()\n",
    "    \n",
    "    # Step 3: Wait to ensure Spark UI is updated\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Step 4: Get new jobs\n",
    "    jobs_after = get_all_jobs()\n",
    "    new_jobs = [j for j in jobs_after if j[\"jobId\"] not in jobs_before]\n",
    "    \n",
    "    # Step 5: Sum duration of all new jobs\n",
    "    total_duration = 0.0\n",
    "    for job in new_jobs:\n",
    "        start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "        end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "        duration = (end - start).total_seconds()\n",
    "        total_duration += duration\n",
    "        print(f\"  Included Job {job['jobId']} took {duration:.2f}s\")\n",
    "\n",
    "    listcount.append(total_duration)\n",
    "end_all = time.time()\n",
    "# Final average\n",
    "print(\"Average time over 10 queries:\", round(math.fsum(listcount)/len(listcount), 2), \"seconds\")\n",
    "print(listcount)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(listcount):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(listcount):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(listcount):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a8faf-e891-42a7-9966-162979ee67cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_env)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
