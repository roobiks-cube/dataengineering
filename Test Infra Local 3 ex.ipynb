{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40ed66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa8e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"App\") \\\n",
    "    .master(\"spark://master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")# total across executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6e1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "SPARK_UI = \"http://192.168.56.102:4040\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f31a9",
   "metadata": {},
   "source": [
    "## Von hdfs lesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e7d9b-0e6d-4269-9057-7cf2a2cee7d9",
   "metadata": {},
   "source": [
    "10 Wiederholungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa0b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 117 braucht 0.09 Sekunden\n",
      "Job ID: 118 braucht 0.08 Sekunden\n",
      "Job ID: 119 braucht 0.11 Sekunden\n",
      "Job ID: 120 braucht 0.16 Sekunden\n",
      "Job ID: 121 braucht 0.10 Sekunden\n",
      "Job ID: 122 braucht 0.12 Sekunden\n",
      "Job ID: 123 braucht 0.12 Sekunden\n",
      "Job ID: 124 braucht 0.13 Sekunden\n",
      "Job ID: 125 braucht 0.12 Sekunden\n",
      "Job ID: 126 braucht 0.09 Sekunden\n",
      "[0.094, 0.08, 0.114, 0.16, 0.105, 0.123, 0.119, 0.133, 0.12, 0.089]\n",
      "Zeit für die Ausführung der Zelle: 2.98 seconds\n",
      "Mittelwert aller Spark Jobs: 0.11 seconds\n",
      "Median aller Spark Jobs: 0.12 seconds\n",
      "Sdt aller Spark Jobs: 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "listh10 = []\n",
    "\n",
    "start_all = time.time()\n",
    "for i in range(10):\n",
    "    dfrh= spark.read.parquet(\"hdfs:///input/parquet\")\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()[0]\n",
    "    start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    timer = (end - start).total_seconds()\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    latest_job = sorted(job_info, key=lambda j: j[\"submissionTime\"], reverse=True)[0]\n",
    "    listh10.append(timer)\n",
    "    print(f\"Job ID: {latest_job['jobId']} braucht {timer:.2f} Sekunden\")\n",
    "end_all = time.time() \n",
    "\n",
    "print(listh10)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(listh10):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(listh10):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(listh10):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778814b3-5527-490d-88d3-d295a83f9b00",
   "metadata": {},
   "source": [
    "100 Wiederholungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b67bb6-7a93-4f89-b719-68267851eba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 127 took 0.10 seconds\n",
      "Job ID: 128 took 0.11 seconds\n",
      "Job ID: 129 took 0.09 seconds\n",
      "Job ID: 130 took 0.09 seconds\n",
      "Job ID: 131 took 0.35 seconds\n",
      "Job ID: 132 took 0.08 seconds\n",
      "Job ID: 133 took 0.09 seconds\n",
      "Job ID: 134 took 0.07 seconds\n",
      "Job ID: 135 took 0.08 seconds\n",
      "Job ID: 136 took 0.06 seconds\n",
      "Job ID: 137 took 0.11 seconds\n",
      "Job ID: 138 took 0.13 seconds\n",
      "Job ID: 139 took 0.12 seconds\n",
      "Job ID: 140 took 0.13 seconds\n",
      "Job ID: 141 took 0.14 seconds\n",
      "Job ID: 142 took 0.11 seconds\n",
      "Job ID: 143 took 0.12 seconds\n",
      "Job ID: 144 took 0.09 seconds\n",
      "Job ID: 145 took 0.20 seconds\n",
      "Job ID: 146 took 0.13 seconds\n",
      "Job ID: 147 took 0.28 seconds\n",
      "Job ID: 148 took 0.09 seconds\n",
      "Job ID: 149 took 0.08 seconds\n",
      "Job ID: 150 took 0.13 seconds\n",
      "Job ID: 151 took 0.14 seconds\n",
      "Job ID: 152 took 0.13 seconds\n",
      "Job ID: 153 took 0.13 seconds\n",
      "Job ID: 154 took 0.17 seconds\n",
      "Job ID: 155 took 0.14 seconds\n",
      "Job ID: 156 took 0.14 seconds\n",
      "Job ID: 157 took 0.10 seconds\n",
      "Job ID: 158 took 0.14 seconds\n",
      "Job ID: 159 took 0.07 seconds\n",
      "Job ID: 160 took 0.07 seconds\n",
      "Job ID: 161 took 0.07 seconds\n",
      "Job ID: 162 took 0.07 seconds\n",
      "Job ID: 163 took 0.07 seconds\n",
      "Job ID: 164 took 0.12 seconds\n",
      "Job ID: 165 took 0.14 seconds\n",
      "Job ID: 166 took 0.12 seconds\n",
      "Job ID: 167 took 0.10 seconds\n",
      "Job ID: 168 took 0.12 seconds\n",
      "Job ID: 169 took 0.12 seconds\n",
      "Job ID: 170 took 0.12 seconds\n",
      "Job ID: 171 took 0.12 seconds\n",
      "Job ID: 172 took 0.13 seconds\n",
      "Job ID: 173 took 0.12 seconds\n",
      "Job ID: 174 took 0.06 seconds\n",
      "Job ID: 175 took 0.08 seconds\n",
      "Job ID: 176 took 0.06 seconds\n",
      "Job ID: 177 took 0.07 seconds\n",
      "Job ID: 178 took 0.11 seconds\n",
      "Job ID: 179 took 0.12 seconds\n",
      "Job ID: 180 took 0.11 seconds\n",
      "Job ID: 181 took 0.14 seconds\n",
      "Job ID: 182 took 0.12 seconds\n",
      "Job ID: 183 took 0.07 seconds\n",
      "Job ID: 184 took 0.10 seconds\n",
      "Job ID: 185 took 0.13 seconds\n",
      "Job ID: 186 took 0.10 seconds\n",
      "Job ID: 187 took 0.10 seconds\n",
      "Job ID: 188 took 0.13 seconds\n",
      "Job ID: 189 took 0.13 seconds\n",
      "Job ID: 190 took 0.06 seconds\n",
      "Job ID: 191 took 0.07 seconds\n",
      "Job ID: 192 took 0.07 seconds\n",
      "Job ID: 193 took 0.08 seconds\n",
      "Job ID: 194 took 0.11 seconds\n",
      "Job ID: 195 took 0.10 seconds\n",
      "Job ID: 196 took 0.11 seconds\n",
      "Job ID: 197 took 0.12 seconds\n",
      "Job ID: 198 took 0.19 seconds\n",
      "Job ID: 199 took 0.13 seconds\n",
      "Job ID: 200 took 0.12 seconds\n",
      "Job ID: 201 took 0.12 seconds\n",
      "Job ID: 202 took 0.12 seconds\n",
      "Job ID: 203 took 0.08 seconds\n",
      "Job ID: 204 took 0.07 seconds\n",
      "Job ID: 205 took 0.06 seconds\n",
      "Job ID: 206 took 0.07 seconds\n",
      "Job ID: 207 took 0.06 seconds\n",
      "Job ID: 208 took 0.06 seconds\n",
      "Job ID: 209 took 0.08 seconds\n",
      "Job ID: 210 took 0.13 seconds\n",
      "Job ID: 211 took 0.12 seconds\n",
      "Job ID: 212 took 0.12 seconds\n",
      "Job ID: 213 took 0.11 seconds\n",
      "Job ID: 214 took 0.13 seconds\n",
      "Job ID: 215 took 0.12 seconds\n",
      "Job ID: 216 took 0.11 seconds\n",
      "Job ID: 217 took 0.12 seconds\n",
      "Job ID: 218 took 0.16 seconds\n",
      "Job ID: 219 took 0.07 seconds\n",
      "Job ID: 220 took 0.06 seconds\n",
      "Job ID: 221 took 0.07 seconds\n",
      "Job ID: 222 took 0.07 seconds\n",
      "Job ID: 223 took 0.10 seconds\n",
      "Job ID: 224 took 0.12 seconds\n",
      "Job ID: 225 took 0.10 seconds\n",
      "Job ID: 226 took 0.12 seconds\n",
      "[0.102, 0.106, 0.09, 0.085, 0.355, 0.082, 0.088, 0.065, 0.078, 0.062, 0.109, 0.133, 0.122, 0.134, 0.135, 0.111, 0.124, 0.093, 0.203, 0.129, 0.275, 0.09, 0.076, 0.13, 0.136, 0.127, 0.13, 0.169, 0.136, 0.144, 0.103, 0.136, 0.073, 0.069, 0.075, 0.065, 0.072, 0.119, 0.142, 0.116, 0.099, 0.117, 0.12, 0.118, 0.122, 0.126, 0.124, 0.06, 0.081, 0.058, 0.071, 0.114, 0.121, 0.109, 0.143, 0.123, 0.065, 0.095, 0.128, 0.104, 0.097, 0.134, 0.129, 0.062, 0.069, 0.071, 0.078, 0.109, 0.096, 0.113, 0.119, 0.188, 0.129, 0.12, 0.122, 0.121, 0.08, 0.065, 0.056, 0.066, 0.057, 0.06, 0.077, 0.134, 0.12, 0.121, 0.113, 0.127, 0.125, 0.107, 0.119, 0.162, 0.073, 0.057, 0.068, 0.065, 0.103, 0.12, 0.095, 0.116]\n",
      "Zeit für die Ausführung der Zelle: 26.04 seconds\n",
      "Mittelwert aller Spark Jobs: 0.11 seconds\n",
      "Median aller Spark Jobs: 0.11 seconds\n",
      "Sdt aller Spark Jobs: 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "listh100 = []\n",
    "\n",
    "start_all = time.time()\n",
    "for i in range(100):\n",
    "    dfrh= spark.read.parquet(\"hdfs:///input/parquet\")\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()[0]\n",
    "    start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "    timer = (end - start).total_seconds()\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    latest_job = sorted(job_info, key=lambda j: j[\"submissionTime\"], reverse=True)[0]\n",
    "    listh100.append(timer)\n",
    "    print(f\"Job ID: {latest_job['jobId']} took {timer:.2f} seconds\")\n",
    "end_all = time.time()\n",
    "\n",
    "print(listh100)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(listh100):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(listh100):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(listh100):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d06c70",
   "metadata": {},
   "source": [
    "## Count testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e5b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrh= spark.read.parquet(\"hdfs:///input/parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "520ca32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrh.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f9606",
   "metadata": {},
   "source": [
    "Einfacher select und count, ohne Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25658c67-05eb-4a7f-bb03-38a3fc7f84af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 259 took 0.10s\n",
      "  Included Job 258 took 2.74s\n",
      "  Included Job 257 took 5.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 262 took 0.08s\n",
      "  Included Job 261 took 4.77s\n",
      "  Included Job 260 took 6.30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 265 took 0.09s\n",
      "  Included Job 264 took 4.08s\n",
      "  Included Job 263 took 8.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 268 took 0.07s\n",
      "  Included Job 267 took 5.41s\n",
      "  Included Job 266 took 7.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 271 took 0.07s\n",
      "  Included Job 270 took 4.50s\n",
      "  Included Job 269 took 9.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 274 took 0.07s\n",
      "  Included Job 273 took 4.38s\n",
      "  Included Job 272 took 9.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 277 took 0.10s\n",
      "  Included Job 276 took 4.44s\n",
      "  Included Job 275 took 7.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 280 took 0.12s\n",
      "  Included Job 279 took 4.09s\n",
      "  Included Job 278 took 7.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 283 took 0.06s\n",
      "  Included Job 282 took 4.07s\n",
      "  Included Job 281 took 7.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Included Job 286 took 0.09s\n",
      "  Included Job 285 took 4.52s\n",
      "  Included Job 284 took 7.61s\n",
      "Average time over 10 queries: 12.12 seconds\n",
      "[8.619, 11.149000000000001, 12.834, 12.96, 14.207, 14.219, 12.353, 11.338, 11.328, 12.218]\n",
      "Zeit für die Ausführung der Zelle: 143.56 seconds\n",
      "Mittelwert aller Spark Jobs: 12.12 seconds\n",
      "Median aller Spark Jobs: 12.29 seconds\n",
      "Sdt aller Spark Jobs: 1.56 seconds\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "listcount = []\n",
    "\n",
    "start_all = time.time()\n",
    "def get_all_jobs():\n",
    "    app_id = requests.get(f\"{SPARK_UI}/api/v1/applications\").json()[0][\"id\"]\n",
    "    job_info = requests.get(f\"{SPARK_UI}/api/v1/applications/{app_id}/jobs\").json()\n",
    "    return job_info\n",
    "\n",
    "for i in range(10):\n",
    "    # Step 1: Snapshot of current jobs\n",
    "    jobs_before = {j[\"jobId\"] for j in get_all_jobs()}\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT gbifID, COUNT(gbifID) AS count\n",
    "    FROM my_table\n",
    "    WHERE gbifID IS NOT NULL\n",
    "    GROUP BY gbifID\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Create Spark DataFrame\n",
    "    spark_df = spark.sql(query)\n",
    "\n",
    "# Show the result in Spark (default top 20 rows)\n",
    "    spark_df.count()\n",
    "    \n",
    "    # Step 3: Wait to ensure Spark UI is updated\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Step 4: Get new jobs\n",
    "    jobs_after = get_all_jobs()\n",
    "    new_jobs = [j for j in jobs_after if j[\"jobId\"] not in jobs_before]\n",
    "    \n",
    "    # Step 5: Sum duration of all new jobs\n",
    "    total_duration = 0.0\n",
    "    for job in new_jobs:\n",
    "        start = datetime.fromisoformat(job[\"submissionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "        end = datetime.fromisoformat(job[\"completionTime\"].replace(\"GMT\", \"+00:00\"))\n",
    "        duration = (end - start).total_seconds()\n",
    "        total_duration += duration\n",
    "        print(f\"  Included Job {job['jobId']} took {duration:.2f}s\")\n",
    "\n",
    "    listcount.append(total_duration)\n",
    "end_all = time.time()\n",
    "# Final average\n",
    "print(\"Average time over 10 queries:\", round(math.fsum(listcount)/len(listcount), 2), \"seconds\")\n",
    "print(listcount)\n",
    "print(f\"Zeit für die Ausführung der Zelle: {end_all - start_all:.2f} seconds\")\n",
    "print(f\"Mittelwert aller Spark Jobs: {np.mean(listcount):.2f} seconds\")\n",
    "print(f\"Median aller Spark Jobs: {np.median(listcount):.2f} seconds\")\n",
    "print(f\"Sdt aller Spark Jobs: {np.std(listcount):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a8faf-e891-42a7-9966-162979ee67cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
